Model,max_depth,learning_rate,n_estimators,alpha,gamma,subsample,colsample_bytree,Booster,RMSE,Notes
Model 1,6,0.05,1000,,,,,,0.4186,"Baseline"
Model 2,10,0.01,2000,,,,,,0.4117,"Deeper trees, slower learning rate improved RMSE"
Model 3,6,0.01,1000,,,,,,0.4420,"Slower learning rate hurt performance"
Model 4,6,0.01,2000,,,,,,0.4291,"More trees slightly helped"
Model 5,10,0.001,2000,,,,,,0.6034,"Learning rate too low"
Model 6,10,0.1,2000,,,,,,0.4140,"Faster learning rate worked well"
Model 7,7,0.03,1500,,,,,,0.4139,"Balanced depth, rate, and estimators"
Model 8,10,0.01,2000,,0.1,,,,0.4148,"Added gamma"
Model 9,10,0.01,2000,0.5,,,,,0.4116,"Added alpha"
Model 10,10,0.01,2000,,,0.8,,,0.4075,"Added subsample"
Model 11,10,0.01,2000,,,,0.8,,0.4081,"Added colsample_bytree"
Model 12,10,0.01,2000,,,,,dart,-,"Too slow"
Model 13,10,0.01,2000,0.5,,0.8,,,0.4068,"Alpha + subsample improved RMSE"
Model 14,10,0.01,2000,0.1,0.1,0.8,0.8,,0.4052,"Comprehensive tuning"
Model 15,10,0.01,3000,0.1,0.005,0.7,0.5,,0.4012,"More trees, small gamma improved RMSE"
Model 16,10,0.01,5000,0.1,0.005,0.7,0.5,,0.3989,"Further improvement with more trees"
Model 17,10,0.001,5000,0.1,0.005,0.7,0.5,,0.4224,"Learning rate too low negated gains"